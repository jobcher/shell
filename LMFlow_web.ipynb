{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobcher/shell/blob/main/LMFlow_web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lrcgZnHHQUJ"
      },
      "source": [
        "## 下载代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KddK4t-Wsl6n",
        "outputId": "cc62e3a6-fbe5-4d30-f239-dd4d36fcb057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LMFlow'...\n",
            "remote: Enumerating objects: 1006, done.\u001b[K\n",
            "remote: Counting objects: 100% (981/981), done.\u001b[K\n",
            "remote: Compressing objects: 100% (423/423), done.\u001b[K\n",
            "remote: Total 1006 (delta 469), reused 940 (delta 453), pack-reused 25\u001b[K\n",
            "Receiving objects: 100% (1006/1006), 5.33 MiB | 19.56 MiB/s, done.\n",
            "Resolving deltas: 100% (470/470), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OptimalScale/LMFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqU73Xe7tWvO",
        "outputId": "19319d75-4bb0-4b69-a960-cf68772b1766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LMFlow\n"
          ]
        }
      ],
      "source": [
        "%cd ./LMFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGYFFSEDGFLc"
      },
      "source": [
        "## 此步骤将安装运行聊天机器人所需的所有依赖项，大约需要 3 分钟。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvbPyWQvt5BW",
        "outputId": "adbd17ff-eec8-4e4a-84a1-cf768cb8869a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/LMFlow\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting peft@ git+https://github.com/huggingface/peft@df0e1fb\n",
            "  Cloning https://github.com/huggingface/peft (to revision df0e1fb) to /tmp/pip-install-2x0f3z5g/peft_1405dedaec7943438617e40871b29673\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-install-2x0f3z5g/peft_1405dedaec7943438617e40871b29673\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'df0e1fb', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q df0e1fb\n",
            "  Resolved https://github.com/huggingface/peft to commit df0e1fb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trl@ git+https://github.com/lvwerra/trl.git#egg=trl-0.4.1\n",
            "  Cloning https://github.com/lvwerra/trl.git to /tmp/pip-install-2x0f3z5g/trl_4c8e075c70094f01bea2f29c27fb3442\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-install-2x0f3z5g/trl_4c8e075c70094f01bea2f29c27fb3442\n",
            "  Resolved https://github.com/lvwerra/trl.git to commit a2749d9e0c96198486b788875eda3b325f76a5c8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers@c612628\n",
            "  Cloning https://github.com/huggingface/transformers (to revision c612628) to /tmp/pip-install-2x0f3z5g/transformers_ab6d933133ef4227ae6fc157eb5429c1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-install-2x0f3z5g/transformers_ab6d933133ef4227ae6fc157eb5429c1\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'c612628', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q c612628\n",
            "  Resolved https://github.com/huggingface/transformers to commit c612628\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.24.2\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (from lmflow==0.0.1) (2.0.0+cu118)\n",
            "Collecting wandb==0.14.0\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed==0.8.3\n",
            "  Downloading deepspeed-0.8.3.tar.gz (765 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.4/765.4 KB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.9/dist-packages (from lmflow==0.0.1) (2.2.3)\n",
            "Collecting flask_cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (4.65.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (1.4.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (2023.3.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (23.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->lmflow==0.0.1) (9.0.0)\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed==0.8.3->lmflow==0.0.1) (5.9.4)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed==0.8.3->lmflow==0.0.1) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->lmflow==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb==0.14.0->lmflow==0.0.1) (1.4.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.14.0->lmflow==0.0.1) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb==0.14.0->lmflow==0.0.1) (67.6.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.14.0->lmflow==0.0.1) (3.20.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->lmflow==0.0.1) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->lmflow==0.0.1) (3.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask->lmflow==0.0.1) (6.1.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask->lmflow==0.0.1) (2.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from flask->lmflow==0.0.1) (2.2.3)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.9/dist-packages (from flask_cors->lmflow==0.0.1) (1.16.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers@c612628->lmflow==0.0.1) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m771.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->lmflow==0.0.1) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->lmflow==0.0.1) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask->lmflow==0.0.1) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->lmflow==0.0.1) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->lmflow==0.0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->lmflow==0.0.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->lmflow==0.0.1) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->lmflow==0.0.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->lmflow==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->lmflow==0.0.1) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: deepspeed, peft, transformers, trl, pathtools\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776423 sha256=34ee9ec44959e336493c299cc7e93c2847907824a6d00089081f24d6a1e947c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=40532 sha256=13719e10cff6d2ad19a9a7d13bd363d927cf1a177023e5f1d3d755daf0ccb292\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7vc2g7wp/wheels/c2/29/9d/fbdfa853e1645f192e9a7a88afe996d4dc0a0a4f2ab1d729c1\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6858427 sha256=bde5394890a0123fb5b0c486680ed1b9c4ab521b1831d023ec906f22c666a09b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7vc2g7wp/wheels/65/9d/1b/5633348da899b76f9dcd50818d5f37a29471c37d1665e18dd6\n",
            "  Building wheel for trl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=53628 sha256=6fd0917402f3675e947ff14f0337eb22e80276e7c41180031570ae80bc3ae877\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7vc2g7wp/wheels/ab/81/88/2e3ddd7591b397b560da92477ae2578b9b6f16f97a57ef49e1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9624e8ff2bce94242d8846d0a1a10c0d582b7a5e5897f2d45de7cdfeee009fd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built deepspeed peft transformers trl pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, py-cpuinfo, pathtools, ninja, hjson, xxhash, smmap, setproctitle, sentry-sdk, numpy, multidict, frozenlist, docker-pycreds, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, gitdb, aiosignal, transformers, GitPython, flask_cors, aiohttp, wandb, datasets, accelerate, trl, peft, deepspeed, lmflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Running setup.py develop for lmflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 accelerate-0.18.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 deepspeed-0.8.3 dill-0.3.6 docker-pycreds-0.4.0 flask_cors-3.0.10 frozenlist-1.3.3 gitdb-4.0.10 hjson-3.1.0 huggingface-hub-0.13.3 lmflow-0.0.1 multidict-6.0.4 multiprocess-0.70.14 ninja-1.11.1 numpy-1.24.2 pathtools-0.1.2 peft-0.3.0.dev0 py-cpuinfo-9.0.0 responses-0.18.0 sentencepiece-0.1.97 sentry-sdk-1.18.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.2 transformers-4.28.0.dev0 trl-0.4.2.dev0 wandb-0.14.0 xxhash-3.2.0 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp39-cp39-linux_x86_64.whl size=3380656 sha256=7537fb10a02f68a341dd03fcceeaef4c47fe6a4fbc79e0bd86b174006596c2fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/81/9f/43a031fce121c845baca1c5d9a1468cad98208286aa2832de9\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.9/dist-packages (3.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -e .\n",
        "!pip install mpi4py\n",
        "!pip3 install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6AB7ZINBfVa"
      },
      "source": [
        "## 此链接将用于访问网络链接。注意：在成功设置（接下来的两个单元格）后，它才能访问。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBpwXHwjLS_t"
      },
      "source": [
        "## 确保在访问上述内容之前运行此单元格 ` https://xxxxxxxxxxxxx.trycloudflare.com`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfMuOsftw4oj",
        "outputId": "26f832d3-3051-4e03-ea3d-b8406e11d791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-04 00:57:37--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2023.3.1/cloudflared-linux-amd64 [following]\n",
            "--2023-04-04 00:57:37--  https://github.com/cloudflare/cloudflared/releases/download/2023.3.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/71fe9daf-6c03-4d6a-b5ec-644b5f1c3b0d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230404%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230404T005737Z&X-Amz-Expires=300&X-Amz-Signature=91e03b3117d51c881d003ab47bd8ec582b564c8fcc554aadbc6376f7ad3039e8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-04-04 00:57:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/71fe9daf-6c03-4d6a-b5ec-644b5f1c3b0d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230404%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230404T005737Z&X-Amz-Expires=300&X-Amz-Signature=91e03b3117d51c881d003ab47bd8ec582b564c8fcc554aadbc6376f7ad3039e8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35611822 (34M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64.1’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  33.96M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-04-04 00:57:38 (370 MB/s) - ‘cloudflared-linux-amd64.1’ saved [35611822/35611822]\n",
            "\n",
            "nohup: appending output to 'nohup.out'\n",
            "2023-04-04T00:57:26Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2023-04-04T00:57:26Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2023-04-04T00:57:28Z INF +--------------------------------------------------------------------------------------------+\n",
            "2023-04-04T00:57:28Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2023-04-04T00:57:28Z INF |  https://tripadvisor-rental-found-containers.trycloudflare.com                             |\n",
            "2023-04-04T00:57:28Z INF +--------------------------------------------------------------------------------------------+\n",
            "2023-04-04T00:57:28Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2023-04-04T00:57:28Z INF Version 2023.3.1\n",
            "2023-04-04T00:57:28Z INF GOOS: linux, GOVersion: go1.19.3, GoArch: amd64\n",
            "2023-04-04T00:57:28Z INF Settings: map[protocol:quic url:localhost:5000]\n",
            "2023-04-04T00:57:28Z INF Generated Connector ID: 98931f6a-62c3-4e9a-a5a5-27cb4307d549\n",
            "2023-04-04T00:57:28Z INF Autoupdate frequency is set autoupdateFreq=86400000\n",
            "2023-04-04T00:57:28Z INF Initial protocol quic\n",
            "2023-04-04T00:57:28Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2023-04-04T00:57:28Z INF ICMP proxy will use :: as source for IPv6\n",
            "2023-04-04T00:57:28Z WRN The user running cloudflared process has a GID (group ID) that is not within ping_group_range. You might need to add that user to a group within that range, or instead update the range to encompass a group the user is already in by modifying /proc/sys/net/ipv4/ping_group_range. Otherwise cloudflared will not be able to ping this network error=\"Group ID 0 is not between ping group 1 to 0\"\n",
            "2023-04-04T00:57:28Z WRN ICMP proxy feature is disabled error=\"cannot create ICMPv4 proxy: Group ID 0 is not between ping group 1 to 0 nor ICMPv6 proxy: socket: permission denied\"\n",
            "2023-04-04T00:57:28Z INF Starting metrics server on 127.0.0.1:46121/metrics\n",
            "2023/04/04 00:57:28 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n",
            "2023-04-04T00:57:29Z INF Connection 6e5ad146-ded5-497f-a774-1a5e4c580aac registered with protocol: quic connIndex=0 ip=198.41.192.57 location=HKG\n",
            "2023-04-04T00:57:30Z INF Connection ac200fcc-48f2-4f5e-bc47-e28e939e27ac registered with protocol: quic connIndex=1 ip=198.41.200.113 location=SIN\n",
            "2023-04-04T00:57:31Z INF Connection c3c40f5d-44fc-41ae-9d9a-3f321e730326 registered with protocol: quic connIndex=2 ip=198.41.200.13 location=SIN\n",
            "2023-04-04T00:57:32Z INF Connection e215617b-fa6d-4fc6-80ae-8b5602ed749f registered with protocol: quic connIndex=3 ip=198.41.192.67 location=HKG\n",
            "2023-04-04T00:57:38Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2023-04-04T00:57:38Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2023-04-04T00:57:41Z INF +--------------------------------------------------------------------------------------------+\n",
            "2023-04-04T00:57:41Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2023-04-04T00:57:41Z INF |  https://scout-hockey-unless-icq.trycloudflare.com                                         |\n",
            "2023-04-04T00:57:41Z INF +--------------------------------------------------------------------------------------------+\n",
            "2023-04-04T00:57:41Z INF Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "2023-04-04T00:57:41Z INF Version 2023.3.1\n",
            "2023-04-04T00:57:41Z INF GOOS: linux, GOVersion: go1.19.3, GoArch: amd64\n",
            "2023-04-04T00:57:41Z INF Settings: map[protocol:quic url:localhost:5000]\n",
            "2023-04-04T00:57:41Z INF Generated Connector ID: c823a13a-cc3f-4b78-8de1-66d009cb6dba\n",
            "2023-04-04T00:57:41Z INF Autoupdate frequency is set autoupdateFreq=86400000\n",
            "2023-04-04T00:57:41Z INF Initial protocol quic\n",
            "2023-04-04T00:57:41Z INF ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "2023-04-04T00:57:41Z INF ICMP proxy will use :: as source for IPv6\n",
            "2023-04-04T00:57:41Z WRN The user running cloudflared process has a GID (group ID) that is not within ping_group_range. You might need to add that user to a group within that range, or instead update the range to encompass a group the user is already in by modifying /proc/sys/net/ipv4/ping_group_range. Otherwise cloudflared will not be able to ping this network error=\"Group ID 0 is not between ping group 1 to 0\"\n",
            "2023-04-04T00:57:41Z WRN ICMP proxy feature is disabled error=\"cannot create ICMPv4 proxy: Group ID 0 is not between ping group 1 to 0 nor ICMPv6 proxy: socket: permission denied\"\n",
            "2023-04-04T00:57:41Z INF Starting metrics server on 127.0.0.1:35963/metrics\n",
            "2023/04/04 00:57:41 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n",
            "2023-04-04T00:57:42Z INF Connection 87de956a-4729-464b-ab50-d1fdfff9eedc registered with protocol: quic connIndex=0 ip=198.41.200.53 location=SIN\n",
            "2023-04-04T00:57:42Z INF Connection 523435ad-7b7a-4132-9fc1-ceaed8cc00f4 registered with protocol: quic connIndex=1 ip=198.41.192.167 location=HKG\n",
            "2023-04-04T00:57:44Z INF Connection 1c24b315-6fe8-4fdc-ac92-dfd9af7b5e61 registered with protocol: quic connIndex=2 ip=198.41.200.193 location=SIN\n",
            "2023-04-04T00:57:45Z INF Connection 9fa1a307-95c2-40ef-a1e7-64910cb6d871 registered with protocol: quic connIndex=3 ip=198.41.192.27 location=HKG\n"
          ]
        }
      ],
      "source": [
        "# start cf-argo-tunnel (optional)\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x cloudflared-linux-amd64\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url localhost:5000 &\n",
        "!sleep 10 && cat nohup.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP6qlTIzyXPi"
      },
      "outputs": [],
      "source": [
        "%cd service\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5f9Rxnrym6X",
        "outputId": "0b231231-6d68-4c73-b227-fa244f3a0287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-04 00:58:02.717310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-04-04 00:59:08,311] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2023-04-04 00:59:09,370] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n",
            "[2023-04-04 00:59:09,370] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2023-04-04 00:59:09,377] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\n",
            "[2023-04-04 00:59:09,642] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2023-04-04 00:59:09,647] [INFO] [logging.py:93:log_dist] [Rank 0] Creating BF16 optimizer\n",
            "[2023-04-04 00:59:10,011] [INFO] [utils.py:829:see_memory_usage] begin bf16_optimizer\n",
            "[2023-04-04 00:59:10,012] [INFO] [utils.py:830:see_memory_usage] MA 5.13 GB         Max_MA 10.37 GB         CA 10.38 GB         Max_CA 10 GB \n",
            "[2023-04-04 00:59:10,012] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 6.24 GB, percent = 49.2%\n",
            "Using /root/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py39_cu118/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 1.1177828311920166 seconds\n",
            "[2023-04-04 00:59:13,788] [INFO] [utils.py:829:see_memory_usage] end bf16_optimizer\n",
            "[2023-04-04 00:59:13,790] [INFO] [utils.py:830:see_memory_usage] MA 5.13 GB         Max_MA 5.13 GB         CA 10.38 GB         Max_CA 10 GB \n",
            "[2023-04-04 00:59:13,790] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 6.26 GB, percent = 49.3%\n",
            "[2023-04-04 00:59:13,791] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\n",
            "[2023-04-04 00:59:13,792] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2023-04-04 00:59:13,792] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2023-04-04 00:59:13,792] [INFO] [config.py:1022:print]   amp_enabled .................. False\n",
            "[2023-04-04 00:59:13,792] [INFO] [config.py:1022:print]   amp_params ................... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   bfloat16_enabled ............. True\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff87e5458e0>\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   communication_data_type ...... None\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   disable_allgather ............ False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   dump_state ................... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... None\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\n",
            "[2023-04-04 00:59:13,793] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   fp16_auto_cast ............... None\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   fp16_enabled ................. False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   global_rank .................. 0\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   gradient_clipping ............ 0.0\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 1\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   loss_scale ................... 1.0\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   memory_breakdown ............. False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2023-04-04 00:59:13,794] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   optimizer_name ............... None\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   optimizer_params ............. None\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   pld_enabled .................. False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   pld_params ................... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   prescale_gradients ........... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   scheduler_name ............... None\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   scheduler_params ............. None\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   sparse_attention ............. None\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   steps_per_print .............. 2000\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   train_batch_size ............. 1\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  1\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   world_size ................... 1\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   zero_enabled ................. False\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 0\n",
            "[2023-04-04 00:59:13,795] [INFO] [config.py:1007:print_user_config]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_micro_batch_size_per_gpu\": 1, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0004494190216064453 seconds\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:23] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:23] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:25] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:25] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:25] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:25] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:26] \"GET /static/assets/background.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:26] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:26] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:27] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:30] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:33] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:33] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 00:59:44] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:00:12] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:00:49] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:00:49] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:00:51] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:00:51] \"GET /static/assets/background.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:19] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:26] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:27] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:27] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:27] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:27] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:01:50] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:04] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:53] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:58] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:58] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:58] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:59] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:02:59] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:03:11] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:04:13] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:04] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:09] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:09] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:09] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:09] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:10] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:10] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:07:10] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:03] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:24] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:24] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:24] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:24] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:24] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:08:56] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:11] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:23] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:27] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:28] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:28] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:28] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:28] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:41] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:09:58] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:03] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:03] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:04] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:04] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:04] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:25] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:10:42] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:02] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:16] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:17] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:17] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:17] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:11:18] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:12:06] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:18:39] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:18:40] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 01:18:40] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:24:53] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:24:53] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:25:09] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:25:11] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:25:12] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:25:13] \"GET /static/assets/background.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 02:25:16] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 03:44:20] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 04:03:52] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 04:03:53] \"GET /static/utils/vue-spinner.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 04:03:53] \"GET /static/assets/logo.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 04:03:57] \"GET /static/assets/background.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:42] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:42] \"\u001b[36mGET /static/utils/vue-spinner.js HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:42] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:42] \"\u001b[36mGET /static/assets/background.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:43] \"\u001b[33mGET /static/dist/assets/favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:43] \"\u001b[36mGET /static/assets/logo.png HTTP/1.1\u001b[0m\" 304 -\n",
            "127.0.0.1 - - [04/Apr/2023 05:07:54] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spAgig8_3ZsZ",
        "outputId": "4833bb2f-8920-4552-9c3f-bab3fa476bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Apr  4 00:56:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# this cell can be used to check your colab GPU type\n",
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}